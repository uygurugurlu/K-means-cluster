# -*- coding: utf-8 -*-
"""report.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KMWw9RH0F2mnT4U8trDkPFwKKnYjTRoG

Below Cell Imports the required **built-in libraries** that are allowed at homework.
"""

import math
import random
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
from sklearn.metrics import confusion_matrix

"""Next cell defines a function that helps parsing string data to float and handling uncompatible data instances like '.5' and '0.5'."""

# Function that parses the data to prevent from inconsistent data at training and test
# For Example: Iris data had '.5' valued data in it's columns. And it also has '0.5'
# This function handles the difference, and parses both as the same.
# Data also gets casted from String to float.
def makeFloat(lineSplit):
    l = []
    for i in range(len(lineSplit)):
        if i != len(lineSplit) - 1:
            try:
                l.append(float(lineSplit[i]))
            except ValueError:
                l.append(float('0' + lineSplit[i]))

    return l

"""Next cell defines a function that calculates and returns the Euclidean Distance between points x and y."""

# This function calculates and returns the Euclidean Distance between point x and point y.
def calculateEuclidean(x, y):
    distance = math.sqrt(sum([(a - b) ** 2 for a, b in zip(x, y)]))
    return distance

"""Next cell defines a function that evaluates and returns the centroid of the given data."""

# This function calculates and returns the center of the given data.
def calculateCentroid(data):
    cent = []
    for i in range(len(data[0])):
        cent.append((sum(row[i] for row in data)) * 1.0 / len(data))
    return cent

"""Next cell defines a function that selects k random point from given data and returns them."""

# This function selects k random point from data and returns it.
def selectRandomK(data, k):
    l = []
    selected = []
    for i in range(k):
        x = random.randint(0, len(data)-1)
        if x not in selected:
            selected.append(x)
            l.append(data[x])
    return l

"""Next cell defines a function that assigns every data instance that is given to the closest centroid to them. Closest centroid for each data gets calculated by Euclidean Distance between each individual data point and every centroid. And returns the assigned data."""

# This function assigns every data instance that is given to the
# closest centroid to them. Closest centroid for each data gets
# calculated by Euclidean Distance between each individual
# data point and every centroid.
def assignDataToCentroids(centroids, data):
    assignedData = []
    for i in data:
        minDistance = calculateEuclidean(i, centroids[0])
        centroid = 0
        for j in range(len(centroids)):
            euc = calculateEuclidean(i, centroids[j])
            if euc < minDistance:
                minDistance = euc
                centroid = j
        assignedData.append(centroid)
    return assignedData

"""Next cell defines a function that does the recomputing of  centroids after  every data instance isassigned to a centroid. For every centroid this process is done individually."""

# This Function calculates a new centroid by using every data point assigned
# to each centroid, individually.
def recomputeCentroids(data, assignedData, centroids):
    newCentroids = [] # Temporary list to store new centroids
    centroidDataTemp = []
    noCentroitCount = 0
    for i in range(len(centroids)):
        for j in range(len(assignedData)): # Assigned data contains the info of each samples assigned cluster
            if i == assignedData[j]:
                centroidDataTemp.append(data[j])
        if centroidDataTemp:
            newCentroids.append(calculateCentroid(centroidDataTemp))
        else:
            noCentroitCount += 1
        centroidDataTemp = []
    newCentroids += noCentroitCount * [newCentroids[0]]
    return newCentroids

"""Next cell defines the elbow function. For n_cluster values from 1 to 10, calculates sum of squared distances of every sample to its cluster returns them as a list."""

# For n_cluster values from 1 to 10, calculates sum of squared distances of every sample to its cluster
# returns them as a list.
def elbow(train):
    sums = []
    for n_cluster in range(1, 10):
        sum = 0.0
        clf = KMeansClusterClassifier(n_cluster)
        clf.fit(train)
        assignedData = assignDataToCentroids(clf.centroids, train)
        for i in range(len(train)):
            sum += calculateEuclidean(clf.centroids[assignedData[i]], train[i])
        sums.append(sum)
    return sums

"""Next cell defines KMeansClusterClassifier class with its initializer method, fit method and predict method."""

class KMeansClusterClassifier:
    def __init__(self, n_cluster):
        self.n_cluster = n_cluster
        self.centroids = []

    def fit(self, X):
        self.centroids = selectRandomK(X, self.n_cluster)
        prevCentroids = []
        assignedData = assignDataToCentroids(self.centroids, X)
        while prevCentroids != self.centroids:
            prevCentroids = self.centroids
            self.centroids = recomputeCentroids(X, assignedData, self.centroids)
            assignedData = assignDataToCentroids(self.centroids, X)

    def predict(self, X):
        minDistance = calculateEuclidean(self.centroids[0], X)
        minCentroid = 0
        for i in range(len(self.centroids)):
            if minDistance > calculateEuclidean(self.centroids[i], X):
                minDistance = calculateEuclidean(self.centroids[i], X)
                minCentroid = i
        return minCentroid

"""Next cell splits the dataset in to %80 as train data and %20 as test data."""

f = open("iris.csv", "r")
data = []
dataLabel = []
f.readline()
lines = f.readlines()
count = 0
for line in lines:
    count += 1
    lineSplit = line.split(',')
    data.append(makeFloat(lineSplit))
    dataLabel.append(lineSplit[-1])
trainSize = count * 80.0 / 100.0
train = data[:int(trainSize)]
trainLabel = dataLabel[:int(trainSize)]
test = data[int(trainSize):]
testLabel = dataLabel[int(trainSize):]
print(trainLabel)

"""Next cell uses elbow function to see the breakdowns between points."""

# print(kmeansC.predict([4.678947368421052, 3.08421052631579, 1.3789473684210527, 0.20000000000000007]))
print(elbow(train))
plt.plot([1, 2, 3, 4, 5, 6 ,7 ,8 ,9], elbow(train))
plt.show() #as seen on the plot, optimal n is 2

"""As it can be seen from the graph above, the breakdown between 1 and 2 seems high, since the first centroids are randomly generated. The reason for this, as we said, is that the first centroids are formed randomly, so they can occur far away.

Considering this, we observe that the breakdown between 2 and 3 is the largest breakdown. For this reason, we can choose our n_Cluster number as 3.
"""

kmeansC = KMeansClusterClassifier(3)
kmeansC.fit(train)

"""Next cell plots the 3D Cluster Plot of the KMeansClusterClassifier with 3 Clustered form."""

X = train

fig = plt.figure(figsize=(4, 3))
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
assignedData = assignDataToCentroids(kmeansC.centroids, train)
x0 = [row[0] for row in train]
x3 = [row[3] for row in train]
x2 = [row[2] for row in train]
ax.scatter(x3, x0, x2,
            c=assignedData, edgecolor='k')

ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])
ax.set_xlabel('Petal width')
ax.set_ylabel('Sepal length')
ax.set_zlabel('Petal length')
ax.dist = 12

fig.show()

"""As it can be seen above Yellow ones are Virginica, Teal ones are Versicolor and purple ones are Setosa.

Below cell plots the confusion matrix for 3 clusters.
"""

# Prep
k_labels = assignedData  # Get cluster labels
k_labels_matched = np.empty_like(k_labels)
# For each cluster label...
truth = []
temp = ["\"Setosa\"\n","\"Versicolor\"\n","\"Virginica\"\n"]
for i in range(len(trainLabel)):
  truth.append(temp.index(trainLabel[i]))
print(truth)
print(k_labels)
for k in np.unique(k_labels):

    # ...find and assign the best-matching truth label
    match_nums = [np.sum((k_labels==k)*(truth==t)) for t in np.unique(truth)]
    k_labels_matched[k_labels==k] = np.unique(truth)[np.argmax(match_nums)]

cm = confusion_matrix(truth, k_labels_matched)

# Plot confusion matrix
plt.imshow(cm, interpolation='none', cmap='Blues')
for (i, j), z in np.ndenumerate(cm):
    plt.text(j, i, z, ha='center', va='center')
plt.xlabel("kmeans label")
plt.ylabel("truth label")
plt.show()

"""Now at the below cell, We will display the confusion matrix for our classifier."""

from sklearn.metrics import classification_report
y_true = truth
y_pred = k_labels_matched
target_names = ['Setosa', 'Virginica', 'Versicolor']
print(classification_report(y_true, y_pred, target_names=target_names))

from sklearn import metrics
import numpy as np
import matplotlib.pyplot as plt

y_true = truth# true labels
y_probas = k_labels_matched# predicted results
fpr, tpr, thresholds = metrics.roc_curve(y_true, y_probas, pos_label=0)

# Print ROC curve
plt.plot(fpr,tpr)
plt.show() 

# Print AUC
auc = np.trapz(tpr,fpr)
print('AUC:', auc)

"""BELOW IS FOR COMPARISON REASONS: HW1 Results will be displayed as SciKit Decision Tree Since I didn't do Hw1's last part."""

from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier()
X_train = train
X_test = test
y_train = trainLabel
y_test = testLabel
print(len(testLabel))
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))